<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>SpeakRight — Pronunciation Trainer (Vosk small model)</title>
<meta name="description" content="SpeakRight: paste text, read sentence-by-sentence, get accent-friendly pronunciation corrections with Vosk in-browser."/>
<style>
  :root{--bg:#f5f7fb;--card:#fff;--accent:#0a54ff;--good:#1e8f3e;--warn:#ff9800;--bad:#d32f2f;--muted:#55626f;--radius:12px}
  html,body{height:100%;margin:0;font-family:Inter,system-ui, -apple-system, "Segoe UI", Roboto, "Helvetica Neue", Arial}
  body{background:var(--bg);color:var(--muted);padding:18px;display:flex;justify-content:center}
  .wrap{width:100%;max-width:940px}
  header{display:flex;justify-content:space-between;align-items:center;margin-bottom:14px}
  header h1{margin:0;font-size:20px;color:#0b2540}
  .card{background:var(--card);border-radius:var(--radius);padding:16px;box-shadow:0 6px 18px rgba(10,37,64,0.06)}
  textarea#inputBox{width:100%;min-height:140px;border-radius:10px;border:1px solid #e6edf6;padding:12px;font-size:15px;resize:vertical}
  .controls{display:grid;grid-template-columns:1fr 160px;gap:12px;margin-top:12px}
  .btn{background:var(--accent);color:white;border:0;padding:12px;border-radius:10px;font-weight:700;cursor:pointer}
  .btn.secondary{background:#eef2ff;color:var(--accent);border:1px solid #dbe9ff}
  #sentencesBox{margin-top:16px;display:block}
  .sentence{padding:10px 12px;border-radius:10px;margin-bottom:10px;background:linear-gradient(180deg,#fff,#fbfdff);border:1px solid #eff5fa;font-size:16px}
  .token{display:inline-block;margin:3px 6px 3px 0;padding:4px 6px;border-radius:6px}
  .token.good{background:#e6f6ea;color:var(--good);font-weight:600}
  .token.unclear{background:#fff6e6;color:var(--warn);font-weight:600}
  .token.bad{background:#ffecec;color:var(--bad);font-weight:700;text-decoration:underline}
  #correctionBox{display:none;margin-top:12px;padding:12px;border-radius:8px;background:#fff7f7;border:1px solid #ffd6d6;color:var(--bad)}
  #progressWrap{margin-top:12px}
  #progressBar{height:12px;background:#e6eef6;border-radius:8px;overflow:hidden}
  #progressFill{height:100%;width:0%;background:linear-gradient(90deg,var(--accent), #58d6b9)}
  .ad-placeholder{margin-top:12px;padding:12px;border-radius:10px;background:#f3f6ff;text-align:center;color:#5a6f9b;border:1px dashed #dfe9ff}
  .model-loader{margin-top:12px;padding:10px;background:#fff;border-radius:8px;border:1px solid #eef4ff;color:#234}
</style>
</head>
<body>
  <div class="wrap">
    <header>
      <div>
        <h1>SPEAKRIGHT</h1>
        <small>Accent-friendly pronunciation trainer — runs in your browser (Vosk).</small>
      </div>
      <div class="small">Model: small • Voice: American (male-ish)</div>
    </header>

    <div class="card">
      <label for="inputBox" style="font-weight:700;color:#0b2540">Paste your English text</label>
      <textarea id="inputBox" placeholder="Paste a paragraph or practice sentences here..."></textarea>

      <div class="controls">
        <button id="processBtn" class="btn">Split Into Sentences</button>
        <button id="clearBtn" class="btn secondary">Clear</button>
      </div>

      <div id="sentencesBox" style="margin-top:14px"></div>
      <div id="correctionBox"></div>

      <div id="progressWrap">
        <div class="small">Progress</div>
        <div id="progressBar"><div id="progressFill"></div></div>
      </div>

      <div style="display:flex;gap:12px;margin-top:12px;">
        <button id="startBtn" class="btn" style="flex:1;display:none">Start Reading This Sentence</button>
        <button id="nextBtn" class="btn secondary" style="display:none">Skip / Mark as Done</button>
      </div>

      <div id="modelLoader" class="model-loader">Model status: <span id="modelStatus">not loaded</span></div>

      <div class="ad-placeholder" id="adPlaceholder">Ad placeholder — insert AdSense after approval</div>
      <div style="margin-top:10px;font-size:13px;color:#6b7a90">Tip: Use Chrome (desktop or Android) for best mic & performance.</div>
    </div>
  </div>

<script>
/*
  Vosk in-browser integration + accent-friendly sentence-by-sentence evaluation.

  Requirements:
   - Put Vosk runtime files at /vosk/ (vosk.js, vosk.wasm, worker if needed)
   - Put the small model files at /model-small/
   - This script loads the model, records audio via WebAudio, sends PCM to Vosk,
     receives text (partial/final), then uses sliding-window + Levenshtein alignment
     to mark tokens correct/unclear/wrong and plays correction once.

  Note: If your vosk runtime or model filenames differ, update paths below.
*/

(async function(){
  // Config: update if your files are in different locations
  const VOSK_JS = '/vosk/vosk.js';        // must exist in repo
  const VOSK_WASM = '/vosk/vosk.wasm';    // must exist in repo
  const MODEL_PATH = '/model-small';      // model folder in repo

  // UI elements
  const inputBox = document.getElementById('inputBox');
  const processBtn = document.getElementById('processBtn');
  const clearBtn = document.getElementById('clearBtn');
  const sentencesBox = document.getElementById('sentencesBox');
  const correctionBox = document.getElementById('correctionBox');
  const progressFill = document.getElementById('progressFill');
  const startBtn = document.getElementById('startBtn');
  const nextBtn = document.getElementById('nextBtn');
  const modelStatus = document.getElementById('modelStatus');

  // State
  let sentences = [];
  let currentIndex = 0;
  let awaitingRetry = false;
  let modelReady = false;
  let recognizer = null;
  let microphoneStream = null;
  let audioCtx = null;
  let sourceNode = null;
  let processorNode = null;
  let voskWorker = null;

  // Load Vosk runtime script dynamically
  async function loadVoskRuntime(){
    modelStatus.textContent = 'loading runtime...';
    await new Promise((res, rej) => {
      const s = document.createElement('script');
      s.src = VOSK_JS;
      s.onload = () => res();
      s.onerror = (e) => rej(new Error('Failed to load Vosk runtime: ' + e.message));
      document.head.appendChild(s);
    });
    modelStatus.textContent = 'runtime loaded';
  }

  // Initialize Vosk worker + load model (worker approach is common)
  async function initVoskModel(){
    modelStatus.textContent = 'initializing model (this may take a few seconds)...';
    // Use the Vosk API global (vosk) loaded by vosk.js
    if (typeof Vosk === 'undefined' && typeof vosk === 'undefined') {
      // some builds expose "Vosk" others "vosk"
      modelStatus.textContent = 'Vosk runtime not found';
      throw new Error('Vosk runtime not found');
    }
    const VoskLib = window.Vosk || window.vosk; // whichever is available

    // Create worker using the runtime's createWorker or similar factory
    // Many vosk-browser builds provide Vosk.createWorker({modelUrl, wasmUrl, ...})
    // We'll try to use a standard pattern; if your vosk.js exposes a different API adapt accordingly.
    modelStatus.textContent = 'loading model files...';
    try {
      // If your Vosk runtime provides createModel or createWorker
      if (VoskLib.createModel) {
        // Some versions: createModel(modelUrl). Then createRecognizer(model)
        const model = await VoskLib.createModel(MODEL_PATH);
        recognizer = await model.createRecognizer({model: model, grammar: null});
      } else if (VoskLib.createWorker) {
        // create worker-based recognizer (preferred)
        voskWorker = await VoskLib.createWorker({
          modelUrl: MODEL_PATH,
          wasmUrl: VOSK_WASM,
          // optional: workerUrl: '/vosk/worker.js'
          // progress callback:
          onProgress: (p) => modelStatus.textContent = 'model load: ' + Math.round(p*100) + '%'
        });
        recognizer = voskWorker; // abstract common name
      } else {
        // fallback: try new VoskLib.Model + VoskLib.Recognizer (older API)
        if (VoskLib.Model) {
          const model = new VoskLib.Model(MODEL_PATH);
          recognizer = new VoskLib.Recognizer({model: model, sampleRate: 16000});
        } else {
          throw new Error('Unsupported Vosk runtime API in this bundle.');
        }
      }
      modelReady = true;
      modelStatus.textContent = 'model ready';
    } catch (err) {
      modelStatus.textContent = 'model load failed: ' + (err.message || err);
      console.error('Vosk init error', err);
      throw err;
    }
  }

  // Utility: normalize text for comparison
  function normalizeText(s){
    return String(s || '')
      .replace(/\u2019/g, "'")
      .replace(/[\u2013\u2014]/g, "-")
      .replace(/[^\w'\- ]+/g, '')
      .replace(/\s+/g,' ')
      .trim()
      .toLowerCase();
  }

  function levenshtein(a,b){
    if(!a) return b.length;
    if(!b) return a.length;
    const m = a.length, n = b.length;
    const dp = Array.from({length:m+1}, () => Array(n+1).fill(0));
    for(let i=0;i<=m;i++) dp[i][0]=i;
    for(let j=0;j<=n;j++) dp[0][j]=j;
    for(let i=1;i<=m;i++){
      for(let j=1;j<=n;j++){
        dp[i][j] = Math.min(dp[i-1][j]+1, dp[i][j-1]+1, dp[i-1][j-1] + (a[i-1]===b[j-1]?0:1));
      }
    }
    return dp[m][n];
  }

  function alignmentCost(targetWords, spokenSlice){
    const m = targetWords.length, n = spokenSlice.length, INF = 1e9;
    const dp = Array.from({length:m+1}, () => Array(n+1).fill(INF));
    dp[0][0]=0;
    for(let i=0;i<=m;i++){
      for(let j=0;j<=n;j++){
        if(i<m && j<n){
          const cost = levenshtein(targetWords[i], spokenSlice[j]);
          if(dp[i+1][j+1] > dp[i][j] + cost) dp[i+1][j+1] = dp[i][j] + cost;
        }
        if(i<m){
          if(dp[i+1][j] > dp[i][j] + 1) dp[i+1][j] = dp[i][j] + 1;
        }
        if(j<n){
          if(dp[i][j+1] > dp[i][j] + 1) dp[i][j+1] = dp[i][j] + 1;
        }
      }
    }
    return dp[m][n];
  }

  function alignWordsGetMapping(targetWords, spokenSlice){
    const m = targetWords.length, n = spokenSlice.length, INF = 1e9;
    const dp = Array.from({length:m+1}, () => Array(n+1).fill(INF));
    const parent = Array.from({length:m+1}, () => Array(n+1).fill(null));
    dp[0][0]=0;
    for(let i=0;i<=m;i++){
      for(let j=0;j<=n;j++){
        if(i<m && j<n){
          const cost = levenshtein(targetWords[i], spokenSlice[j]);
          if(dp[i+1][j+1] > dp[i][j] + cost){
            dp[i+1][j+1] = dp[i][j] + cost;
            parent[i+1][j+1] = {i,j,type:'match'};
          }
        }
        if(i<m){
          if(dp[i+1][j] > dp[i][j] + 1){
            dp[i+1][j] = dp[i][j] + 1;
            parent[i+1][j] = {i,j,type:'skipTarget'};
          }
        }
        if(j<n){
          if(dp[i][j+1] > dp[i][j] + 1){
            dp[i][j+1] = dp[i][j] + 1;
            parent[i][j+1] = {i,j,type:'insertSpoken'};
          }
        }
      }
    }
    let i=m, j=n, actions=[];
    while(i>0||j>0){
      const p = parent[i][j];
      if(!p) break;
      actions.push(p);
      i=p.i; j=p.j;
    }
    actions.reverse();
    const alignment = new Array(m).fill(null);
    let tIdx=0, sIdx=0;
    for(const a of actions){
      if(a.type==='match'){ alignment[tIdx]=sIdx; tIdx++; sIdx++; }
      else if(a.type==='skipTarget'){ tIdx++; }
      else if(a.type==='insertSpoken'){ sIdx++; }
    }
    return alignment;
  }

  // Render sentences into the DOM
  function renderSentences(){
    sentencesBox.innerHTML = '';
    sentences.forEach((s,i) => {
      sentencesBox.insertAdjacentHTML('beforeend', `<div class="sentence" id="sentence-${i}">${escapeHtml(s)}</div>`);
    });
  }
  function escapeHtml(s){ return String(s||'').replace(/&/g,'&amp;').replace(/</g,'&lt;').replace(/>/g,'&gt;'); }

  // Buttons
  processBtn.addEventListener('click', ()=>{
    const text = inputBox.value || '';
    if(!text.trim()) return alert('Please paste text first.');
    sentences = text.match(/[^.!?]+[.!?]?/g) || [];
    currentIndex = 0;
    renderSentences();
    updateProgress();
    startBtn.style.display = 'inline-block';
    nextBtn.style.display = 'inline-block';
    correctionBox.style.display = 'none';
    awaitingRetry = false;
  });

  clearBtn.addEventListener('click', ()=>{
    inputBox.value = '';
    sentences = [];
    sentencesBox.innerHTML = '';
    startBtn.style.display = 'none';
    nextBtn.style.display = 'none';
    correctionBox.style.display = 'none';
    progressFill.style.width = '0%';
  });

  nextBtn.addEventListener('click', ()=>{ moveToNextSentence(); });

  // Start listening (this uses WebAudio + Vosk worker)
  async function startListeningForCurrent(){
    if (!modelReady){
      alert('Model not ready yet. Wait until "model status" shows ready.');
      return;
    }
    if(!sentences.length) return alert('No sentences. Paste text and click split.');
    const target = sentences[currentIndex];
    if(!target) return;

    // ask for microphone
    try {
      microphoneStream = await navigator.mediaDevices.getUserMedia({ audio: true });
    } catch (e) {
      return alert('Microphone permission denied or not available.');
    }

    // create WebAudio context and capture PCM frames at 16kHz
    audioCtx = new (window.AudioContext || window.webkitAudioContext)();
    sourceNode = audioCtx.createMediaStreamSource(microphoneStream);

    // create processor (script processor has wide support)
    // buffer size 4096 is okay; we convert from audioCtx.sampleRate -> 16000
    const bufferSize = 4096;
    processorNode = audioCtx.createScriptProcessor(bufferSize, 1, 1);

    // Prepare conversion parameters
    const inputSampleRate = audioCtx.sampleRate;
    const targetRate = 16000;

    // Start sending audio chunks to Vosk worker/recognizer
    processorNode.onaudioprocess = (e) => {
      const inputData = e.inputBuffer.getChannelData(0);
      // downsample to 16k
      const down = downsampleBuffer(inputData, inputSampleRate, targetRate);
      // convert to Int16
      const int16 = floatTo16BitPCM(down);
      // send to recognizer API
      try {
        if (voskWorker && typeof voskWorker.postMessage === 'function') {
          // worker API: post PCM -> {cmd: 'process', data: ...} or custom; check your vosk.worker API.
          // Many createWorker abstractions expose 'send' or 'post' functions; try common ones:
          if (voskWorker.send) voskWorker.send({cmd:'process',buffer:int16});
          else if (voskWorker.postMessage) voskWorker.postMessage({cmd:'process',buffer:int16});
          else if (recognizer && recognizer.acceptWaveform) recognizer.acceptWaveform(int16);
        } else if (recognizer && recognizer.acceptWaveform) {
          recognizer.acceptWaveform(int16);
        }
      } catch(err){
        console.error('send audio error', err);
      }
    };

    // connect nodes
    sourceNode.connect(processorNode);
    processorNode.connect(audioCtx.destination);

    // Start a short listener: accumulate audio for ~2.5s or until silence
    // We'll collect final transcript from recognizer after stopping.
    modelStatus.textContent = 'Recording... speak the sentence clearly and pause.';
    // simple timer to capture about 3 seconds; you can extend for longer sentences
    await sleep(2500); // record ~2.5s
    stopRecordingAndProcess(target);
  }

  // Stop audio capture and ask the model for recognition result
  async function stopRecordingAndProcess(originalSentence){
    // disconnect audio nodes
    if (processorNode) { processorNode.disconnect(); processorNode.onaudioprocess = null; processorNode = null; }
    if (sourceNode) { try{ sourceNode.disconnect(); }catch(e){} sourceNode = null; }
    try { if (audioCtx) await audioCtx.close(); } catch(e){}
    audioCtx = null;

    // stop mic permission stream tracks
    if (microphoneStream) {
      microphoneStream.getTracks().forEach(t => t.stop());
      microphoneStream = null;
    }

    modelStatus.textContent = 'Processing audio with model...';

    // Ask recognizer or worker for final result
    // Support multiple runtimes: worker-based or direct recognizer
    try {
      let finalText = '';
      if (voskWorker && voskWorker.getResult) {
        // some worker wrappers have getResult()
        const res = await voskWorker.getResult();
        finalText = res.text || '';
      } else if (voskWorker && voskWorker.postMessage) {
        // If worker expects message, we send finish and wait for response; implement simple promise
        finalText = await sendFinishToWorkerAndGetResult();
      } else if (recognizer && recognizer.finalResult) {
        const r = await recognizer.finalResult();
        finalText = r.text || '';
      } else if (recognizer && recognizer.getResult) {
        const r = await recognizer.getResult();
        finalText = r.text || '';
      } else {
        console.warn('No standard result API found on recognizer/worker; trying to read partial via recognizer.result');
        if (recognizer && recognizer.result) {
          finalText = recognizer.result;
        }
      }

      modelStatus.textContent = 'Model result ready';
      console.log('Final recognized text:', finalText);
      if (!finalText) {
        markAllUnclear();
        awaitingRetry = true;
        return;
      }
      // Evaluate the recognized finalText against original sentence
      handleRecognitionResult(originalSentence, finalText.toLowerCase());
    } catch (err) {
      console.error('recognition final error', err);
      modelStatus.textContent = 'Recognition failed';
      markAllUnclear();
      awaitingRetry = true;
    }
  }

  // If worker communicates via messages we need this helper: sends finish and wait for response
  function sendFinishToWorkerAndGetResult(){
    return new Promise((resolve, reject) => {
      // this duck-typing assumes worker will postMessage({cmd:'result', text:...})
      // Implement a one-time message listener
      const handler = (e) => {
        try {
          const d = e.data || {};
          if (d && (d.cmd === 'result' || d.result)) {
            window.removeEventListener('message', handler);
            resolve(d.text || d.result || '');
          }
        } catch(e){}
      };
      window.addEventListener('message', handler);
      // send finish
      try {
        if (voskWorker && voskWorker.postMessage) voskWorker.postMessage({cmd:'finish'});
        else reject(new Error('vosk worker not postMessage'));
      } catch(e){ reject(e); }
      // timeout if no response
      setTimeout(()=>{ window.removeEventListener('message', handler); resolve(''); }, 4000);
    });
  }

  // handle recognized text -> alignment -> highlight -> correction
  function handleRecognitionResult(originalSentence, spokenRaw){
    const targetNorm = normalizeText(originalSentence);
    const spokenNorm = normalizeText(spokenRaw);
    console.log('TARGET:', targetNorm);
    console.log('SPOKEN:', spokenNorm);

    const targetWords = targetNorm.split(/\s+/).filter(Boolean);
    const spokenWords = spokenNorm.split(/\s+/).filter(Boolean);

    if (!spokenWords.length) { markAllUnclear(); awaitingRetry = true; return; }

    // sliding-window best slice
    const n = spokenWords.length;
    let best = {cost: Infinity, start:0, len: Math.min(n, targetWords.length)};
    const minLen = Math.max(1, targetWords.length - 4);
    const maxLen = Math.min(n, targetWords.length + 4);
    for (let start=0; start<n; start++){
      for (let len=minLen; len<=maxLen; len++){
        if (start + len > n) break;
        const slice = spokenWords.slice(start, start+len);
        const cost = alignmentCost(targetWords, slice);
        if (cost < best.cost) best = {cost, start, len};
      }
    }
    if (best.cost === Infinity) { markAllUnclear(); awaitingRetry = true; return; }

    const spokenSlice = spokenWords.slice(best.start, best.start + best.len);
    const mapping = alignWordsGetMapping(targetWords, spokenSlice);

    // build out and determine first wrong
    let firstWrongIndex = -1;
    let outHTML = '';
    for (let i=0;i<targetWords.length;i++){
      const originalToken = getOriginalToken(originalSentence, i);
      const sIdx = mapping[i];
      if (sIdx === null || sIdx === undefined) {
        outHTML += `<span class="token unclear">${escapeHtml(originalToken)}</span> `;
        if (firstWrongIndex === -1) firstWrongIndex = i;
        continue;
      }
      const spokenWord = spokenSlice[sIdx];
      const dist = levenshtein(targetWords[i], spokenWord);
      const tolerance = Math.max(1, Math.floor(targetWords[i].length * 0.6)); // accent-friendly
      if (dist <= tolerance) outHTML += `<span class="token good">${escapeHtml(originalToken)}</span> `;
      else { outHTML += `<span class="token bad">${escapeHtml(originalToken)}</span> `; if (firstWrongIndex === -1) firstWrongIndex = i; }
    }

    const el = document.getElementById(`sentence-${currentIndex}`);
    if (el) el.innerHTML = outHTML;

    if (firstWrongIndex !== -1){
      const spokenDetected = (spokenSlice[mapping[firstWrongIndex]] || '(not detected)');
      const correctToken = getOriginalToken(originalSentence, firstWrongIndex);
      showCorrection(spokenDetected, correctToken);
      speakCorrectWordOnce(correctToken);
      awaitingRetry = true;
      return;
    }

    // success
    correctionBox.style.display = 'none';
    awaitingRetry = false;
    moveToNextSentence();
  }

  function markAllUnclear(){
    const original = sentences[currentIndex] || '';
    const words = (original.trim().split(/\s+/) || []);
    const out = words.map(w=>`<span class="token unclear">${escapeHtml(w)}</span>`).join(' ');
    const el = document.getElementById(`sentence-${currentIndex}`);
    if (el) el.innerHTML = out;
    correctionBox.style.display = 'block';
    correctionBox.textContent = 'No speech detected or not understood. Please try again.';
  }

  function showCorrection(spoken, correctWord){
    correctionBox.style.display = 'block';
    correctionBox.innerHTML = `You said: <b>${escapeHtml(spoken)}</b><br>Correct: <b>${escapeHtml(correctWord)}</b>`;
  }

  function speakCorrectWordOnce(word){
    try { if (speechSynthesis.speaking) speechSynthesis.cancel(); } catch(e){}
    const utt = new SpeechSynthesisUtterance(word);
    const voices = speechSynthesis.getVoices() || [];
    let chosen = voices.find(v => v.lang && v.lang.startsWith('en-US') && /male|man|david|john|daniel|tom/i.test(v.name));
    if (!chosen) chosen = voices.find(v => v.lang && v.lang.startsWith('en-US'));
    if (chosen) utt.voice = chosen;
    utt.rate = 0.95;
    speechSynthesis.speak(utt);
  }

  function getOriginalToken(originalSentence, idx){
    const tokens = originalSentence.trim().split(/\s+/);
    return tokens[idx] || '';
  }

  function moveToNextSentence(){
    currentIndex++;
    if (currentIndex >= sentences.length){
      updateProgress();
      alert('Finished all sentences — great job!');
      return;
    }
    updateProgress();
    const el = document.getElementById(`sentence-${currentIndex}`);
    if (el) el.scrollIntoView({behavior:'smooth', block:'center'});
  }

  function updateProgress(){
    const pct = Math.round((currentIndex / Math.max(1, sentences.length)) * 100);
    progressFill.style.width = pct + '%';
  }

  // helpers: downsample float32 to targetRate 16000 and convert to Int16
  function downsampleBuffer(buffer, sampleRate, outSampleRate) {
    if (outSampleRate === sampleRate) return buffer;
    const ratio = sampleRate / outSampleRate;
    const outLength = Math.round(buffer.length / ratio);
    const out = new Float32Array(outLength);
    let offsetResult = 0;
    let offsetBuffer = 0;
    while (offsetResult < outLength) {
      const nextOffsetBuffer = Math.round((offsetResult + 1) * ratio);
      // average the samples in between
      let accum = 0, count = 0;
      for (let i = offsetBuffer; i < nextOffsetBuffer && i < buffer.length; i++) {
        accum += buffer[i];
        count++;
      }
      out[offsetResult] = count > 0 ? accum / count : 0;
      offsetResult++;
      offsetBuffer = nextOffsetBuffer;
    }
    return out;
  }
  function floatTo16BitPCM(float32Array) {
    const l = float32Array.length;
    const buffer = new Int16Array(l);
    for (let i = 0; i < l; i++) {
      let s = Math.max(-1, Math.min(1, float32Array[i]));
      buffer[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
    }
    return buffer;
  }
  function sleep(ms){ return new Promise(r=>setTimeout(r, ms)); }

  // Initialize: load runtime + model automatically but non-blocking
  (async function bootstrap(){
    try {
      await loadVoskRuntime();          // loads vosk.js
      modelStatus.textContent = 'runtime loaded, loading model...';
      await initVoskModel();            // loads model into worker/recognizer
      modelStatus.textContent = 'model ready';
    } catch (err) {
      console.error('Vosk init failed', err);
      modelStatus.textContent = 'model load failed';
    }
  })();

  // expose start function for UI: starts listening and records 2.5s then processes
  window.startListeningForCurrent = startListeningForCurrent;

  // initial UI hide
  startBtn.style.display = 'none';
  nextBtn.style.display = 'none';

  // show start/next when model ready (poll)
  const waitForReady = setInterval(()=>{
    if (modelReady) {
      startBtn.style.display = 'inline-block';
      nextBtn.style.display = 'inline-block';
      clearInterval(waitForReady);
    }
  }, 500);

})(); // end iife
</script>
</body>
</html>
